packageSet <- c("car", "abind", "aplpack", "colorspace", "effects", "Hmisc",
"leaps", "zoo", "lmtest", "mvtnorm", "multcomp", "relimp", "rgl", "RODBC",
"clv", "rpart.plot", "flexclust", "e1071", "sem", "Rcmdr","foreign","tree","rpart","rattle","ipred","randomForest","dplyr","sqldf","genalg","corrplot","caret","nnet","RSNNS","NeuralNetTools","devtools","kohonen","tidyverse","caTools")
install.packages(packageSet)
rm(packageSet)
packageSet <- c("car", "abind", "aplpack", "colorspace", "effects", "Hmisc",
"leaps", "zoo", "lmtest", "mvtnorm", "multcomp", "relimp", "rgl", "RODBC",
"clv", "rpart.plot", "flexclust", "e1071", "sem", "Rcmdr","foreign","tree","rpart","rattle","ipred","randomForest","dplyr","sqldf","genalg","corrplot","caret","nnet","RSNNS","NeuralNetTools","devtools","kohonen","tidyverse","caTools")
install.packages(packageSet)
rm(packageSet)
install.packages("/Users/michalchruszczewski/Documents/R-packages/BCA_0.9-3.tar.gz", repos=NULL, type= "source")
packageSet <- c("car", "abind", "aplpack", "colorspace", "effects", "Hmisc",
"leaps", "zoo", "lmtest", "mvtnorm", "multcomp", "relimp", "rgl", "RODBC",
"clv", "rpart.plot", "flexclust", "e1071", "sem", "Rcmdr","foreign","tree","rpart","rattle","ipred","randomForest","dplyr","sqldf","genalg","corrplot","caret","nnet","RSNNS","NeuralNetTools","devtools","kohonen","tidyverse","caTools")
install.packages(packageSet)
rm(packageSet)
install.packages("/Users/michalchruszczewski/Documents/R-packages/BCA_0.9-3.tar.gz", repos=NULL, type= "source")
install.packages("/Users/michalchruszczewski/Documents/R-packages/RcmdrPlugin.BCA_0.9-8.tar.gz",repos=NULL, type= "source" )
library(ipred)
SN <- read.csv("http://jolej.linuxpl.info/BD_DU.csv", sep=";")
names(DATA)<-c("size","cost","fited","nc","class")
library(ipred)
SN <- read.csv("http://jolej.linuxpl.info/BD_DU.csv", sep=";")
names(SN)<-c("size","cost","fited","nc","class")
#Zbi?r SN zawiera podobne dane jak ten omawiany na zaj?ciach dodano jednak jesszcze
# jedn? zmienn? obja?niaj?c? BC, kt?ra w skali od 1 do 6 opisuje atrakcyjno??
# nieruchomo?ci wzgl?dem cechy blisko centrum. Prosz? zbudowa? drzewa losowe i nauczy? je
# je klasyfikowa? mieszkania wed?ug tego zbioru.
SN
packageSet <- c("car", "abind", "aplpack", "colorspace", "effects", "Hmisc",
"leaps", "zoo", "lmtest", "mvtnorm", "multcomp", "relimp", "rgl", "RODBC",
"clv", "rpart.plot", "flexclust", "e1071", "sem", "Rcmdr","foreign","tree","rpart","rattle","ipred","randomForest","dplyr","sqldf","genalg","corrplot","caret","nnet","RSNNS","NeuralNetTools","devtools","kohonen","tidyverse","caTools")
install.packages(packageSet)
rm(packageSet)
install.packages("/Users/michalchruszczewski/Documents/Inżynieria Danych- Big Data/R- predykcje/R-packages/BCA_0.9-3.tar.gz", repos=NULL, type= "source")
install.packages("/Users/michalchruszczewski/Documents/Inżynieria Danych- Big Data/R- predykcje/R-package/RcmdrPlugin.BCA_0.9-8.tar.gz",repos=NULL, type= "source" )
install.packages(packageSet)
packageSet <- c("car", "abind", "aplpack", "colorspace", "effects", "Hmisc",
"leaps", "zoo", "lmtest", "mvtnorm", "multcomp", "relimp", "rgl", "RODBC",
"clv", "rpart.plot", "flexclust", "e1071", "sem", "Rcmdr","foreign","tree","rpart","rattle","ipred","randomForest","dplyr","sqldf","genalg","corrplot","caret","nnet","RSNNS","NeuralNetTools","devtools","kohonen","tidyverse","caTools")
install.packages(packageSet)
rm(packageSet)
install.packages("/Users/michalchruszczewski/Documents/Inżynieria Danych- Big Data/R- predykcje/R-packages/BCA_0.9-3.tar.gz", repos=NULL, type= "source")
install.packages("/Users/michalchruszczewski/Documents/Inżynieria Danych- Big Data/R- predykcje/R-package/RcmdrPlugin.BCA_0.9-8.tar.gz",repos=NULL, type= "source" )
library(ipred)
SN <- read.csv("http://jolej.linuxpl.info/BD_DU.csv", sep=";")
SN <- DATA
library(ipred)
SN <- read.csv("http://jolej.linuxpl.info/BD_DU.csv", sep=";")
DATA <- SN
names(DATA)<-c("size","cost","fited","nc","class")
#Zbi?r SN zawiera podobne dane jak ten omawiany na zaj?ciach dodano jednak jesszcze
# jedn? zmienn? obja?niaj?c? BC, kt?ra w skali od 1 do 6 opisuje atrakcyjno??
# nieruchomo?ci wzgl?dem cechy blisko centrum. Prosz? zbudowa? drzewa losowe i nauczy? je
# je klasyfikowa? mieszkania wed?ug tego zbioru.
DATA
library(ipred)
SN <- read.csv("http://jolej.linuxpl.info/BD_DU.csv", sep=";")
DATA <- SN
names(DATA)<-c("size","cost","fited","nc","class")
#Zbi?r SN zawiera podobne dane jak ten omawiany na zaj?ciach dodano jednak jesszcze
# jedn? zmienn? obja?niaj?c? NC, kt?ra w skali od 1 do 6 opisuje atrakcyjno??
# nieruchomo?ci wzgl?dem cechy blisko centrum. Prosz? zbudowa? drzewa losowe i nauczy? je
# je klasyfikowa? mieszkania wed?ug tego zbioru.
X <- DATA[, c("size","cost","fited","nc")]
Y <- DATA$class
model_rf <- rf(x=X, y=Y, ntree= 100, type= 'class')
library(ipred)
SN <- read.csv("http://jolej.linuxpl.info/BD_DU.csv", sep=";")
DATA <- SN
names(DATA)<-c("size","cost","fited","nc","class")
#Zbi?r SN zawiera podobne dane jak ten omawiany na zaj?ciach dodano jednak jesszcze
# jedn? zmienn? obja?niaj?c? NC, kt?ra w skali od 1 do 6 opisuje atrakcyjno??
# nieruchomo?ci wzgl?dem cechy blisko centrum. Prosz? zbudowa? drzewa losowe i nauczy? je
# je klasyfikowa? mieszkania wed?ug tego zbioru.
X <- DATA[, c("size","cost","fited","nc")]
Y <- DATA$class
model_rf <- randomForest(x=X, y=Y, ntree= 100)
ibrary(ipred)
library(ipred)
SN <- read.csv("http://jolej.linuxpl.info/BD_DU.csv", sep=";")
DATA <- SN
names(DATA)<-c("size","cost","fited","nc","class")
#Zbi?r SN zawiera podobne dane jak ten omawiany na zaj?ciach dodano jednak jesszcze
# jedn? zmienn? obja?niaj?c? NC, kt?ra w skali od 1 do 6 opisuje atrakcyjno??
# nieruchomo?ci wzgl?dem cechy blisko centrum. Prosz? zbudowa? drzewa losowe i nauczy? je
# je klasyfikowa? mieszkania wed?ug tego zbioru.
X <- DATA[, c("size","cost","fited","nc")]
Y <- DATA$class
model_rf <- randomForest::randomForest(x=X, y=Y, ntree= 100)
yes
y
library(ipred)
# Wczytaj dane
SN <- read.csv("http://jolej.linuxpl.info/BD_DU.csv", sep = ";")
# Zmień nazwy kolumn na "size", "cost", "fited", "nc", "class"
names(SN) <- c("size", "cost", "fited", "nc", "class")
# Przekształć "class" na zmienną czynnikową
SN$class <- factor(SN$class)
# Przygotowanie danych do modelu
X <- SN[, c("size", "cost", "fited", "nc")]  # Zmienne niezależne
Y <- SN$class  # Zmienna zależna
# Zbudowanie drzewa losowego
model_rf <- randomForest::randomForest(x = X, y = Y, ntree = 100)
model_rf
set.seed(123)  # Ustawienie ziarna, aby eksperyment był powtarzalny
# Wyznaczanie indeksów dla danych uczących
train_index <- sample(1:nrow(SN), nrow(SN)*0.8)
# Podział danych na uczące i testowe
train_data <- SN[train_index, ]
test_data <- SN[-train_index, ]
# Przygotowanie danych do modelu
X_train <- train_data[, c("size", "cost", "fited", "nc")]  # Zmienne niezależne
Y_train <- train_data$class  # Zmienna zależna
# Zbudowanie drzewa losowego na danych uczących
model_rf <- randomForest::randomForest(x = X_train, y = Y_train, ntree = 100)
# Teraz możesz przetestować swój model na danych testowych
X_test <- test_data[, c("size", "cost", "fited", "nc")]  # Zmienne niezależne
Y_test <- test_data$class  # Zmienna zależna
# Przewidywanie i ocena modelu
Y_pred <- predict(model_rf, X_test)
confusionMatrix(Y_pred, Y_test)
set.seed(123)  # Ustawienie ziarna, aby eksperyment był powtarzalny
# Wyznaczanie indeksów dla danych uczących
train_index <- sample(1:nrow(SN), nrow(SN)*0.8)
# Podział danych na uczące i testowe
train_data <- SN[train_index, ]
test_data <- SN[-train_index, ]
# Przygotowanie danych do modelu
X_train <- train_data[, c("size", "cost", "fited", "nc")]  # Zmienne niezależne
Y_train <- train_data$class  # Zmienna zależna
# Zbudowanie drzewa losowego na danych uczących
model_rf <- randomForest::randomForest(x = X_train, y = Y_train, ntree = 100)
# Teraz możesz przetestować swój model na danych testowych
X_test <- test_data[, c("size", "cost", "fited", "nc")]  # Zmienne niezależne
Y_test <- test_data$class  # Zmienna zależna
# Przewidywanie i ocena modelu
Y_pred <- predict(model_rf, X_test)
Y_pred
library(caret)
set.seed(123)  # Ustawienie ziarna, aby eksperyment był powtarzalny
# Wyznaczanie indeksów dla danych uczących
train_index <- sample(1:nrow(SN), nrow(SN)*0.8)
# Podział danych na uczące i testowe
train_data <- SN[train_index, ]
test_data <- SN[-train_index, ]
# Przygotowanie danych do modelu
X_train <- train_data[, c("size", "cost", "fited", "nc")]  # Zmienne niezależne
Y_train <- train_data$class  # Zmienna zależna
# Zbudowanie drzewa losowego na danych uczących
model_rf <- randomForest::randomForest(x = X_train, y = Y_train, ntree = 100)
# Teraz możesz przetestować swój model na danych testowych
X_test <- test_data[, c("size", "cost", "fited", "nc")]  # Zmienne niezależne
Y_test <- test_data$class  # Zmienna zależna
# Przewidywanie i ocena modelu
Y_pred <- predict(model_rf, X_test)
confusionMatrix(Y_pred, Y_test)
library (randomForest)
# Zbudowanie drzewa losowego na danych uczących
model_rf <- randomForest::randomForest(x = X_train, y = Y_train, ntree = 100)
# Wybór i wyświetlenie jednego drzewa; np. pierwszego drzewa w lesie
plot(getTree(model_rf, k=1, labelVar=TRUE))
model_rf
# Wybór i wyświetlenie jednego drzewa; np. pierwszego drzewa w lesie
getTree(model_rf, k=1, labelVar=TRUE)
# Wybór i wyświetlenie jednego drzewa; np. pierwszego drzewa w lesie
rpart.plot(getTree(model_rf, k=1, labelVar=TRUE))
# Wybór i wyświetlenie jednego drzewa; np. pierwszego drzewa w lesie
library(rpart.plot)
rpart.plot(getTree(model_rf, k=1, labelVar=TRUE))
# Wybór i wyświetlenie jednego drzewa; np. pierwszego drzewa w lesie
library(rpart)
rpart.plot(getTree(model_rf, k=1, labelVar=TRUE))
tree1_indices <- which(model_rf$inbag[, 1] == 1)
# Stwórz drzewo decyzyjne `rpart` na tych samych danych
model_rpart <- rpart(class ~ size + cost + fited + nc,
data=train_data[tree1_indices, ],
method="class")
# Wybór i wyświetlenie jednego drzewa; np. pierwszego drzewa w lesie
library(rpart)
tree1_indices <- which(model_rf$inbag[, 1] == 1)
model_rpart <- rpart(class ~ size + cost + fited + nc,
data=train_data[tree1_indices, ],
method="class")
model_rpart <- rpart(class ~ size + cost + fited + nc, data = train_data, method = "class")
# Wizualizacja drzewa decyzyjnego
rpart.plot(model_rpart, type = 4, extra = 101)
library(rpart)
tree1_indices <- which(model_rf$inbag[, 2] == 2)
# Stwórz drzewo decyzyjne `rpart` na tych samych danych
model_rpart <- rpart(class ~ size + cost + fited + nc, data = train_data, method = "class")
# Wizualizacja drzewa decyzyjnego
rpart.plot(model_rpart, type = 4, extra = 101)
library(rpart)
tree1_indices <- which(model_rf$inbag[, 2] == 2)
# Stwórz drzewo decyzyjne `rpart` na tych samych danych
model_rpart <- rpart(class ~ size + cost + fited + nc, data = train_data, method = "class")
# Wizualizacja drzewa decyzyjnego
rpart.plot(model_rpart, type = 4, extra = 101)
# Wybór i wyświetlenie jednego drzewa; np. pierwszego drzewa w lesie
library(rpart)
tree1_indices <- which(model_rf$inbag[, 2] == 2)
# Stwórz drzewo decyzyjne `rpart` na tych samych danych
model_rpart <- rpart(class ~ size + cost + fited + nc, data = train_data, method = "class")
# Wizualizacja drzewa decyzyjnego
rpart.plot(model_rpart, type = 5, extra = 101)
# Wizualizacja drzewa decyzyjnego
rpart.plot(model_rpart, type = 4, extra = 100)
# Wybór i wyświetlenie jednego drzewa; np. pierwszego drzewa w lesie
library(rpart)
tree1_indices <- which(model_rf$inbag[, 3] == 1)
# Stwórz drzewo decyzyjne `rpart` na tych samych danych
model_rpart <- rpart(class ~ size + cost + fited + nc, data = train_data, method = "class")
# Wizualizacja drzewa decyzyjnego
rpart.plot(model_rpart, type = 4, extra = 100)
library(rpart)
tree1_indices <- which(model_rf$inbag[, 99] == 1)
# Stwórz drzewo decyzyjne `rpart` na tych samych danych
model_rpart <- rpart(class ~ size + cost + fited + nc, data = train_data, method = "class")
# Wizualizacja drzewa decyzyjnego
rpart.plot(model_rpart, type = 4, extra = 100)
library(rpart)
tree1_indices <- which(model_rf$inbag[, 98] == 1)
# Stwórz drzewo decyzyjne `rpart` na tych samych danych
model_rpart <- rpart(class ~ size + cost + fited + nc, data = train_data, method = "class")
# Wizualizacja drzewa decyzyjnego
rpart.plot(model_rpart, type = 4, extra = 100)
library (randomForest)
# Zbudowanie drzewa losowego na danych uczących
model_rf <- randomForest::randomForest(x = X_train, y = Y_train, ntree = 100)
# Wybór i wyświetlenie jednego drzewa; np. pierwszego drzewa w lesie
library(rpart)
tree1_indices <- which(model_rf$inbag[, 98] == 1)
# Stwórz drzewo decyzyjne `rpart` na tych samych danych
model_rpart <- rpart(class ~ size + cost + fited + nc, data = train_data, method = "class")
# Wizualizacja drzewa decyzyjnego
rpart.plot(model_rpart, type = 4, extra = 100)
library(ipred)
SN <- read.csv("http://jolej.linuxpl.info/SN.csv", sep=";")
DATA<-SN
names(DATA)<-c("size","cost","fited","class")
DATA$class<-factor(DATA$class)
mod <- bagging(class~., data=DATA, nbagg=150)
a<-predict(mod, newdata=DATA)
library(caret)
confusionMatrix(factor(a),factor(DATA$class))
# Podstawienie danych ze zbioru walidacyjnego
SNV <- read.csv("http://jolej.linuxpl.info/SNV.csv", sep=";")
names(SNV)<-c("size","cost","fited","class")
SNV$class<-factor(SNV$class)
b<-predict(mod, newdata=SNV)
confusionMatrix(factor(b),factor(SNV$class))
packageSet <- c("car", "abind", "aplpack", "colorspace", "effects", "Hmisc",
"leaps", "zoo", "lmtest", "mvtnorm", "multcomp", "relimp", "rgl", "RODBC",
"clv", "rpart.plot", "flexclust", "e1071", "sem", "Rcmdr","foreign","tree","rpart","rattle","ipred","randomForest","dplyr","sqldf","genalg","corrplot","caret","nnet","RSNNS","NeuralNetTools","devtools","kohonen","tidyverse","caTools")
install.packages(packageSet)
rm(packageSet)
install.packages("/Users/michalchruszczewski/Documents/Inżynieria Danych- Big Data/R- predykcje/R-packages/BCA_0.9-3.tar.gz", repos=NULL, type= "source")
packageSet <- c("car", "abind", "aplpack", "colorspace", "effects", "Hmisc",
"leaps", "zoo", "lmtest", "mvtnorm", "multcomp", "relimp", "rgl", "RODBC",
"clv", "rpart.plot", "flexclust", "e1071", "sem", "Rcmdr","foreign","tree","rpart","rattle","ipred","randomForest","dplyr","sqldf","genalg","corrplot","caret","nnet","RSNNS","NeuralNetTools","devtools","kohonen","tidyverse","caTools")
install.packages(packageSet)
rm(packageSet)
install.packages("/Users/michalchruszczewski/Documents/Inżynieria Danych- Big Data/R- predykcje/R-packages/BCA_0.9-3.tar.gz", repos=NULL, type= "source")
install.packages("/Users/michalchruszczewski/Documents/Inżynieria Danych- Big Data/R- predykcje/R-package/RcmdrPlugin.BCA_0.9-8.tar.gz",repos=NULL, type= "source" )
install.packages(packageSet)
source("~/Documents/Inżynieria Danych- Big Data/R- predykcje/R-packages/install_packages.R")
install.packages(packageSet)
rm(list=ls())
gc(full= TRUE)
setwd(~/Documents/Inżynieria Danych- Big Data/Projekt końcowy/R- case study)
setwd('~/Documents/Inżynieria Danych- Big Data/Projekt końcowy/R- case study')
data= read.csv('PredMaint.csv')
data <- read.csv('PredMaint.csv')
View(data)
data <- read.csv('PredMaint.csv', sep = ';')
df <- read.csv('PredMaint.csv', sep = ';')
rm(list=ls())
gc(full= TRUE)
setwd('~/Documents/Inżynieria Danych- Big Data/Projekt końcowy/R- case study')
df <- read.csv('PredMaint.csv', sep = ';')
str(df)
data <- data.table::data.table(df)
View(data)
View(df)
dim(data)
library(data.table)
rm(list=ls())
gc(full= TRUE)
setwd('~/Documents/Inżynieria Danych- Big Data/Projekt końcowy/R- case study')
df <- read.csv('PredMaint.csv', sep = ';')
data <- data.table::data.table(df)
data= data[v1 >1 ]
data= data[V_1 >1 ]
View(data)
data_check= data[V_2 >16 ]
unique(data$Y)
nunique(data$Y)
unique(data$Y)
class(data$Y)
data$Y= data$Y - 1
table(data$Y)
install.packages('FastDummies')
install.packages('fastDummies')
library(fastDummies)
data.dum= dummy_cols(data)
View(data.dum)
data.dum= dummy_cols(data, remove_selected_columns = TRUE)
View(data.dum)
data.dum= dummy_cols(data, remove_selected_columns = TRUE, remove_first_dummy = T)
#Linear Probability Model
pm.lm= lm(Y~ ., data= data.dum)
pm.lm
summary(pm.lm)
data.dum
41.5/2.5
1500/2.5
15/2.5
2.5*15
37.5+1+12+1+5
hist(pm.lm$fitted.values)
pm.glm = glm(Y~ ., data = data.dum, family = binomial(link='logit'))
hist(pm.glm$fitted.values)
summary(pm.glm)
install.packages("neuralnet")
library(neuralnet)
pm.ann <- neuralnet(Y ~ ., data = data.dum, hidden= c(3, 2))
hist(pm.ann$net.result)
hist(pm.ann$net.result[[1]])
pm.ann <- neuralnet(Y ~ ., data = data.dum, hidden= c(3, 3))
hist(pm.ann$net.result[[1]])
hist(pm.ann$net.result[1]])
hist(pm.ann$net.result[1])
pm.ann <- neuralnet(Y ~ ., data = data.dum, hidden= c(3, 2))
hist(pm.ann$net.result[1])
table(pm.ann$net.result[[1]])
#Linear Probability Model
pm.lm <- lm(Y~ ., data= data.dum, remove_selected_columns = TRUE, remove_first_dummy = T)
pm.ann <- neuralnet(Y ~ ., data = df.dum, hidden= c(3, 2))
df.dum<- dummy_cols(df, remove_selected_columns = TRUE, remove_first_dummy = T)
pm.ann <- neuralnet(Y ~ ., data = df.dum, hidden= c(3, 2))
hist(pm.ann$net.result[1])
hist(pm.ann$net.result[[1]])
for (i in 1: ncol(df.dum)){
x= df.dum[,i]
x=(x-min(x))/(max(x)-min(x))
df.dum[,i]=x
}
pm.ann <- neuralnet(Y ~ ., data = df.dum, hidden= c(3, 2))
hist(pm.ann$net.result[[1]])
pm.ann <- neuralnet(Y ~ ., data = df.dum, hidden= c(3, 2), linear.output = F)
hist(pm.ann$net.result[[1]])
table(pm.ann$net.result[[1]])
