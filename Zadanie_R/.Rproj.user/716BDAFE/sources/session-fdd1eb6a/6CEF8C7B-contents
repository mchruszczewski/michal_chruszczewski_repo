install.packages("renv")

library(ROSE)
library(dplyr)
library(smotefamily)
library(dplyr)
library(corrplot)
library(gmodels)
library(klaR)
library(randomForest)

#REMOVE ALL OBJECTS AND CLEAN WORKING ENVIRONMENT:
rm(list=ls())
gc(full= TRUE)

#DATA LOADING:
trainset<-"~/Documents/Inżynieria Danych- Big Data/R- predykcje/r-predykcje/GNU R predykcje- styczeń/train_student.csv"
newset<-"~/Documents/Inżynieria Danych- Big Data/R- predykcje/r-predykcje/GNU R predykcje- styczeń/test_student.csv"
base.set<-read.csv(trainset)
new.set<-read.csv(newset)


#DATA INSPECTION AND TRANSFORMATION (Here is a place for your ideas!):
str(base.set)
str(new.set)

dim(base.set)
dim(new.set)

head(base.set)
head(new.set)

summary(base.set)
summary(new.set)

#DATA INSPECTION

# 1. Pre-defined charts

hist(base.set$BirthYear,main="BirthYear",xlab="Year",ylab="")
hist(base.set$LicenseYear,main="LicenseYear",xlab="Year",ylab="")
hist(base.set$CarBrand,main="CarBrand",xlab="Brand",ylab="")
hist(base.set$CarYear,main="CarYear",xlab="Year",ylab="")
hist(base.set$CarEngine,main="CarEngine",xlab="Engine",ylab="")
hist(base.set$EngineCap,main="EngineCap",xlab="Capacity",ylab="")
hist(base.set$CarValue,main="CarValue",xlab="Value",ylab="")
hist(base.set$AssistanceYears,main="AssistanceYears",xlab="Years",ylab="")
table(base.set$Sex)
table(base.set$Accidents)
table(base.set$NextAccident)

# 2. My charts

# Korelacja między kolumnami


df<- na.omit(base.set)
cor_matrix <- cor(df, method = "spearman")
corrplot(cor_matrix,method='ellipse')

# Sprawdzanie outlierów na box plotach

for (i in 1:ncol(base.set)) {
  boxplot(base.set[[i]],ylab=names(base.set)[i])
}

#tabela kontygencji zmienne kategorialne

df<- base.set

factor_columns <- c('Sex', 'CarType', 'Accidents', 'CarBrand','NextAccident')
contingent_tables <- list()

for (col in factor_columns) {
  if(col != "NextAccident") { # Pominięcie, gdyż nie chcemy porównywać kolumny z samą sobą
    table <- CrossTable(df$NextAccident, df[[col]],
                        prop.c= FALSE, prop.chisq= FALSE, prop.t=FALSE,
                        dnn = c('NextAccident', col)) # Dodatkowe opcje dla czytelności
    contingent_tables[[col]] <- table
  }
}

#Car Engine Histogram for BMW
hist(base.set$BirthYear[base.set$CarBrand=="1"],main="CarEngine BMW",xlab="BirthYear",ylab="")
#Car Engine Histogram for Toyota
hist(base.set$BirthYear[base.set$CarBrand=="2"],main="BirthYear Toyota",xlab="BirthYear",ylab="")

#Group by car variables
matching_columns <- grep("^Car", names(base.set), value = TRUE)
df_brands<- subset(base.set, select= c(matching_columns, 'EngineCap','Accidents','NextAccident'))
df_brands<- na.omit(df_brands)
number_of_accidents <- sum(df_brands$NextAccident == 1)


check_function <- function(df, column, number_of_accidents) {
  df_grouped <- df %>%
    group_by(!!sym(column)) %>%
    summarize(percentage = sum(NextAccident == 1) / number_of_accidents)
  return(df_grouped)
}

# Group by car variables
matching_columns <- grep("^Car", names(base.set), value = TRUE)
df_brands <- subset(base.set, select = c(matching_columns, 'EngineCap', 'Accidents', 'NextAccident'))
df_brands <- na.omit(df_brands)
number_of_accidents <- sum(df_brands$NextAccident == 1)



list_cars <- lapply(names(df_brands), function(v) {
  df <- df_brands
  df <- df %>%
    mutate(column = v)
  df_to_add <- check_function(df, v, number_of_accidents)
  return(df_to_add)
})
  

# Tworzenie wykresów słupkowych za pomocą funkcji lapply()
lapply(list_cars, function(df) {
  column_name <- names(df)[[1]]  # Uzyskanie nazwy kolumny
  barplot(df$percentage, main = paste("Barplot for", column_name), names.arg = df[[column_name]], xlab = column_name, ylab = "Percentage")
})

#Group by driver variables

matching_columns <- c('BirthYear','Sex','LicenseYear','AssistanceYears','Accidents','NextAccident')
df_drivers <-subset(base.set, select = matching_columns)
df_drivers<- na.omit(df_drivers)

list_drivers <- lapply(names(df_drivers), function(v) {
  df <- df_drivers
  df <- df %>%
    mutate(column = v)
  df_to_add <- check_function(df, v, number_of_accidents)
  return(df_to_add)
})

# Tworzenie wykresów słupkowych za pomocą funkcji lapply()
lapply(list_drivers, function(df) {
  column_name <- names(df)[[1]]  # Uzyskanie nazwy kolumny
  barplot(df$percentage, main = paste("Barplot for", column_name), names.arg = df[[column_name]], xlab = column_name, ylab = "Percentage")
})




#####################################
# Place for your data inspection!
# Place for your data cleaning!
# Place for your data transformation!
#####################################



#sprawdzanie outlierów na box plotach

for (i in 1:ncol(base.set)) {
  boxplot(base.set[[i]],ylab=names(base.set)[i])
}

#usuwanie outlierów BirthYear, CarYear, CarEngine, EngineCap, AssistanceYears

nazwy_kolumn_outlier<- c('BirthYear', 'CarYear', 'CarEngine', 'EngineCap', 'AssistanceYears')


df= base.set
df<- na.omit(df)
df$BirthYear<- 2024- df$BirthYear
df$CarYear<- 2024- df$CarYear


outlier_function <- function(df,kolumna){
  Q1 <- quantile(df[[kolumna]], 0.25)
  Q3 <- quantile(df[[kolumna]], 0.75)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  df <- df[df[[kolumna]] > lower_bound & df[[kolumna]] < upper_bound, ]
  return(df)
}



for (kolumna in nazwy_kolumn_outlier){
  df <- outlier_function(df,kolumna)
}

#sprawdzanie outlierów na box plotach po usunięciu outlierów
for (i in 1:ncol(df)) {
  boxplot(df[[i]],ylab=names(df)[i])
}


corrplot(cor(base.set),method='ellipse')



# Czyszczenie danych, inżynieria cech


# 1 możliwość 

preprocess_data <- function(df) {
  current_year <- 2024
  df <- na.omit(df)
  
  df <- df %>%
    mutate(LicenseYear = round(LicenseYear),
           EngineCap = round(EngineCap),
           BirthYear = current_year - round(BirthYear),
           CarYear = current_year - CarYear,
           brand_accident = paste(CarBrand, Accidents, sep = "_"),
           type_accident = paste(CarType, Accidents, sep = "_")) %>%
    dplyr::select(-CarBrand, -Accidents, -AssistanceYears, -CarType) %>%
    mutate(across(c(Sex, type_accident,brand_accident, NextAccident), as.factor))
  
  # Normalizacja zmiennych liczbowych
   numeric_vars <- c("LicenseYear", "EngineCap", "BirthYear", "CarValue", "CarYear","CarEngine")
    df <- df %>%
    mutate(across(all_of(numeric_vars), scale))
  
  # One-hot encoding dla zmiennych kategorycznych za pomocą model.matrix
    df <- df %>%
    mutate_if(is.factor, as.character)  # Konwersja czynników na znaki dla model.matrix
  
    encoded_df <- model.matrix(~ . , data = df)
  
  return(as.data.frame(encoded_df))

}


#2 możliwość
preprocess_data_2 <- function(df, set_c) {
  df[is.na(df$Accidents),] <- mean(df$Accidents)
  current_year <- 2024
  df<- na.omit(df)
  
  df <- df %>%
    mutate(LicenseYear = round(LicenseYear),
           EngineCap = round(EngineCap),
           BirthYear = current_year - round(BirthYear),
           CarYear = current_year - CarYear)
  
  if (set_c == TRUE) {
    df <- df %>%
      mutate(across(c(Sex, Accidents, CarBrand, CarType, NextAccident), as.factor))
  } else {
    df <- df %>%
      mutate(across(c(Sex, Accidents, CarBrand, CarType), as.factor))
  }
  
  return(df)
}

#liczenie naiwnego klasyfikatora bayesowskiego



df <- preprocess_data_2(base.set, set_c=TRUE)
naive_model <- NaiveBayes(NextAccident~ Sex + Accidents + CarBrand + CarType , data= na.omit(df))
print(naive_model$table)





#DATA SET SPLIT ON TRAINING AND TESTING:
SplitDataSet<-function(data.set,training.fraction) {
  random.numbers<-sample.int(nrow(data.set))
  quantiles<-quantile(random.numbers,probs=c(0,training.fraction,1))
  split.labels<-cut(random.numbers,quantiles,include.lowest=T,labels=c("training","test"))
  return(split(data.set,split.labels))
}


new.set <- preprocess_data_2(new.set, set_c = FALSE)
base.set <- preprocess_data_2 (base.set, set_c =TRUE)
split.dataset<-SplitDataSet(base.set,0.7)
class(split.dataset)
str(split.dataset)


for (kolumna in nazwy_kolumn_outlier){
  base.set <- outlier_function(base.set,kolumna)
}

train.set<-split.dataset$train
test.set<-split.dataset$test
class(train.set)
class(test.set)



set.seed(123) # Dla powtarzalności wyników
data_under <- ovun.sample(NextAccident ~ ., data = train.set, method = "under", N = min(table(train.set$NextAccident)) * 2)$data

#BULDING MODEL (Here is a place for your ideas!):

# Model GLM

model.glm <- glm(NextAccident~ .,data = data_under, family = binomial)
print(model.glm)
# Zakładając, że 'model_glm' to twój model regresji logistycznej
predicted_values <- predict(model.glm, newdata= test.set, type = "response")  # Prognozy z modelu
predicted_classes <- ifelse(predicted_values > 0.6, 1, 0)  # Konwertowanie na klasy binarne (0 lub 1)

# Porównanie prognoz z rzeczywistymi wartościami
actual_classes <- test.set$NextAccident  # Załóżmy, że 'df' to twoja ramka danych z rzeczywistymi wartościami
confusion_matrix <- table(Predicted = predicted_classes, Actual = actual_classes)

# Wyświetlenie macierzy pomyłek
print(confusion_matrix)


#Random Forrest


X_train <- data_under[, -which(names(data_under) == "NextAccident")]
y_train <- data_under$NextAccident

rf_model <- randomForest(x = X_train, y = y_train, ntree = 500, mtry = sqrt(ncol(X_train)))

X_test <- test.set[, -which(names(test.set) == "NextAccident")]
y_test <- test.set$NextAccident

# Wyświetlenie ważności cech
waznosc <- importance(rf_model)
waznosc_sorted <- sort(waznosc, decreasing = TRUE)
print(waznosc_sorted)
# Predykcja na zbiorze testowym
predictions <- predict(rf_model, X_test)
# Teraz możesz stworzyć macierz pomyłek z rzeczywistymi etykietami 'y_test' i przewidywanymi klasami 'predicted_classes'
confusionMatrix <- table(Predicted = predictions, Actual = y_test)
print(confusionMatrix)

#CUT-OFF POINT OPTIMISATION:
test.response<-c()
CutOff<-seq(from=0.0,to=1,by=0.02)
incorrect.error<-c()
income<-c()
for(i in 1:length(CutOff)) {
  pred.train<-as.integer(model.glm$fitted.values>=CutOff[i])
  incorrect.error[i]<-sum(abs(pred.train-train.set$NextAccident))/length(train.set$NextAccident)
  income[i]<-length(which(pred.train==0))*1100-length(which((pred.train-train.set$NextAccident)==-1))*5500
}
#Chart of Classification Error
plot(CutOff,incorrect.error,type="l")
MinCutOff.Inc<-which.min(incorrect.error)
points(CutOff[MinCutOff.Inc],incorrect.error[MinCutOff.Inc],col="red",cex=2)
CutOff[MinCutOff.Inc]

#Chart of Revenue
barplot(income)
MinCutOff.Income<-which.max(income)
CutOff[MinCutOff.Income]
points(MinCutOff.Income,income[MinCutOff.Income],col="red",cex=2)

#TEST ON TEST SET:
pred.test<-predict(model,newdata=test.set)
resp.test<-as.integer(pred.test>=CutOff[MinCutOff.Income])

incorrect.test<-sum(abs(as.numeric(resp.test)-test.set$NextAccident))/length(test.set$NextAccident)
income.test<-length(which(as.numeric(resp.test)==0))*1100-length(which((as.numeric(resp.test)-test.set$NextAccident)==-1))*5500

#DECISION MAKING (final decision on Cut-Off):
pred.new.set<-predict(model.glm,newdata=new.set)
resp.test<-as.integer(pred.new.set>=CutOff[MinCutOff.Income])

#SAVING OUTCOME:
write.csv(resp.test,"YourNick_ID_Name.csv",row.names=FALSE,quote=FALSE)
